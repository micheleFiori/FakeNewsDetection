{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from functools import partial\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Load the tsv file in three different pandas dataframe and add a column containing the statement length (as the number of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_table('dataset/train.tsv', names=['ID', 'LABEL', 'STATEMENT', 'TOPICS', 'SPEAKER', 'ROLE', 'CITY', 'PARTY', 'H1', 'H2', 'H3', 'H4', 'H5', 'CONTEXT'])\n",
    "train.insert(loc=3, column='STATEMENT_LEN', value=train.STATEMENT.apply(lambda x: len(x.split())))\n",
    "\n",
    "test = pd.read_table('dataset/test.tsv', names=['ID', 'LABEL', 'STATEMENT', 'TOPICS', 'SPEAKER', 'ROLE', 'CITY', 'PARTY', 'H1', 'H2', 'H3', 'H4', 'H5', 'CONTEXT'])\n",
    "test.insert(loc=3, column='STATEMENT_LEN', value=test.STATEMENT.apply(lambda x: len(x.split())))\n",
    "\n",
    "valid = pd.read_table('dataset/valid.tsv', names=['ID', 'LABEL', 'STATEMENT', 'TOPICS', 'SPEAKER', 'ROLE', 'CITY', 'PARTY', 'H1', 'H2', 'H3', 'H4', 'H5', 'CONTEXT'])\n",
    "valid.insert(loc=3, column='STATEMENT_LEN', value=valid.STATEMENT.apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data\n",
    "Remove statements with excessive word counts as they represent errors (e.g., contain multiple statements together) and remove the statements with a length less than 15 that start with the word \"On\" since they are probably actually titles of articles and as such cannot have truth value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.boxplot(column='STATEMENT_LEN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 3 main outliers from the train set, that are 1606.json, 1993.json and 1720.json\n",
    "ids_to_remove = ['1606.json', '1993.json', '1720.json']\n",
    "train = train[~train.ID.isin(ids_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.boxplot(column='STATEMENT_LEN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.boxplot(column='STATEMENT_LEN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 2 main outliers from the test set, that are 1653.json and 40.json\n",
    "ids_to_remove = ['1653.json', '40.json']\n",
    "test = test[~test.ID.isin(ids_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.boxplot(column='STATEMENT_LEN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from the train, test and validation set the statements that begin with \"on\" and have less than min_len words\n",
    "min_len = 15\n",
    "train = train[~((train.STATEMENT.str.startswith('On')) & (train.STATEMENT_LEN < min_len))]\n",
    "test = test[~((test.STATEMENT.str.startswith('On')) & (test.STATEMENT_LEN < min_len))]\n",
    "valid = valid[~((valid.STATEMENT.str.startswith('On')) & (valid.STATEMENT_LEN < min_len))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the labels\n",
    "1 for true, including 'half-true', 'mostly-true' and 'true'; 0 for false, including 'barely-true', 'false' and 'pants-fire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.LABEL= train.LABEL.apply(lambda x: 0 if x in labels[:3] else 1)\n",
    "test.LABEL= test.LABEL.apply(lambda x: 0 if x in labels[:3] else 1)\n",
    "valid.LABEL= valid.LABEL.apply(lambda x: 0 if x in labels[:3] else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case folding\n",
    "Make all the sentences lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.STATEMENT = train.STATEMENT.str.lower()\n",
    "test.STATEMENT = test.STATEMENT.str.lower()\n",
    "valid.STATEMENT = valid.STATEMENT.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation and stopwords\n",
    "The stopwords can be taken from a custom list of stopwords based on the nltk corpus but with some changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_punctuation(sentence, stop_words):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = True\n",
    "\n",
    "if custom_stopwords:\n",
    "    with open('custom_stopwords', 'r') as f:\n",
    "        stopwords_list = []\n",
    "        for line in f:\n",
    "            if not line.startswith('#'):\n",
    "                stopwords_list.append(line[:-1])\n",
    "else:\n",
    "    stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords_and_punctuation_partial = partial(remove_stopwords_and_punctuation, stop_words=stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.STATEMENT = train.STATEMENT.apply(remove_stopwords_and_punctuation_partial)\n",
    "test.STATEMENT = test.STATEMENT.apply(remove_stopwords_and_punctuation_partial)\n",
    "valid.STATEMENT = valid.STATEMENT.apply(remove_stopwords_and_punctuation_partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Apply Porter stemming to all the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.STATEMENT = train.STATEMENT.apply(lambda x: \" \".join([PorterStemmer().stem(word) for word in nltk.word_tokenize(x)]))\n",
    "test.STATEMENT = test.STATEMENT.apply(lambda x: \" \".join([PorterStemmer().stem(word) for word in nltk.word_tokenize(x)]))\n",
    "valid.STATEMENT = valid.STATEMENT.apply(lambda x: \" \".join([PorterStemmer().stem(word) for word in nltk.word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "Save the pre-processed data in new csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"dataset/prep_train.csv\", index=False)\n",
    "test.to_csv(\"dataset/prep_test.csv\", index=False)\n",
    "valid.to_csv(\"dataset/prep_valid.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
